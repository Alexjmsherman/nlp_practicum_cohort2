{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase (collocation) Detection Exercise\n",
    "\n",
    "###### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agenda\n",
    "1. SpaCy POS phrases\n",
    "2. Gensim Phrases and Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for data, acronyms, and gensim paths\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "\n",
    "DB_PATH = config['DATABASES']['PROJECT_DB_PATH']\n",
    "MATCHED_TEXT_PATH = config['NLP']['MATCHED_TEXT_PATH']\n",
    "CLEANED_TEXT_PATH = config['NLP']['CLEANED_TEXT_PATH']\n",
    "GENSIM_DICTIONARY_PATH = config['NLP']['GENSIM_DICTIONARY_PATH']\n",
    "GENSIM_CORPUS_PATH = config['NLP']['GENSIM_CORPUS_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count # of documents\n",
    "engine = create_engine(DB_PATH)\n",
    "pd.read_sql(\"SELECT COUNT(*) FROM pubmed \", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM pubmed LIMIT 500\", con=engine)\n",
    "\n",
    "# filter to relevant sections\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store section matches in list\n",
    "text = [text for text in df['text']]\n",
    "\n",
    "# review first sentence of a section match\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load spacy nlp model\n",
    "# use 'en' if you don't have the lg model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### collect sentences using SpaCy matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_phrase_model_sents(matcher, doc, i, matches):\n",
    "    # identify matching spans (phrases)\n",
    "    match_id, start, end = matches[i]\n",
    "    \n",
    "    # get sentence with matched term\n",
    "    sent = doc[start:end].sent.text\n",
    "    \n",
    "    # collect matching (cleaned) sents\n",
    "    matched_sents.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### match sentences\n",
    "\n",
    "https://explosion.ai/demos/matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# remove false statement below to run code\n",
    "if 1 == 1:\n",
    "    # match sentences with the word disease or disorder\n",
    "    matched_sents = []\n",
    "    pattern = [[{'LOWER': 'disease'}], [{'LOWER': 'disorder'}]]\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # use *patterns to add more than one pattern at once\n",
    "    matcher.add('disease_disorder', collect_phrase_model_sents, *pattern)\n",
    "\n",
    "    for doc in nlp.pipe(text, disable=['tagger','ner']):    \n",
    "        matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of matches: {} \\n'.format(len(matched_sents)))\n",
    "\n",
    "print('Example Match:')\n",
    "print(matched_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export matched text to avoid repeating processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view path to matched text\n",
    "MATCHED_TEXT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to write the matched text to a .txt file for later use \n",
    "\n",
    "#with open(MATCHED_TEXT_PATH, 'w') as f:\n",
    "#    for line in matched_sents:\n",
    "#        line += '\\n'\n",
    "#        line = line.encode('ascii', errors='ignore').decode('ascii') \n",
    "#        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read matched text\n",
    "with open(MATCHED_TEXT_PATH, 'r') as f:\n",
    "    matched_sents_full = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents_full[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all matched sentences in a dataframe\n",
    "matches_df = pd.DataFrame(matched_sents_full, columns=['sentences'])\n",
    "\n",
    "# remove duplicates\n",
    "matches_df = matches_df.drop_duplicates()\n",
    "\n",
    "# recreate matched_sents (since it takes so long to create on its own)\n",
    "matched_sents = [sent[0].split() for sent in matches_df.values]\n",
    "\n",
    "# view matches\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SpaCy part of speech (POS) to create phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the matched sentence tokens and parse it with SpaCy\n",
    "text = ' '.join(matched_sents[2])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determine which NLP components can be disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_pos(doc, n_tokens=5):\n",
    "    \"\"\" print SpaCy POS information about each token in a provided document \"\"\"\n",
    "    print('{:15} | {:10} | {:10} | {:30}'.format('TOKEN','POS','DEP_','LEFTS'))\n",
    "    for token in doc[0:n_tokens]:\n",
    "        print('{:15} | {:10} | {:10} | {:30}'.format(\n",
    "            token.text, token.head.pos_,token.dep_, str([t.text for t in token.lefts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by named entity recognition (ner)\n",
    "pos_doc = nlp(text, disable=['ner'])\n",
    "view_pos(pos_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by parser\n",
    "pos_doc = nlp(text, disable=['ner','parser'])\n",
    "view_pos(pos_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by tagger\n",
    "pos_doc = nlp(text, disable=['ner','tagger'])\n",
    "view_pos(pos_doc, n_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use explain to define any token.dep_ attributes\n",
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_parsing_labels_url = 'https://spacy.io/api/annotation#dependency-parsing'\n",
    "iframe = '<iframe src={} width=1000 height=400></iframe>'.format(dependency_parsing_labels_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract phrases by identifying tokens describing an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stop words to SpaCy\n",
    "# this enables the .is_stop attribute with common stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy_doc = nlp(text)\n",
    "\n",
    "# show visualization in Jupyter Notebook\n",
    "displacy.render(docs=displacy_doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_phrases(doc):\n",
    "\n",
    "    phrases = [] \n",
    "\n",
    "    doc = nlp(doc, disable=['ner','tagger'])\n",
    "    for token in doc:\n",
    "        direct_object = 'obj' in token.dep_\n",
    "        if direct_object:\n",
    "            # find any dependent terms to the left of (preceeding) the object\n",
    "            for left_term in (t.text for t in token.lefts if not t.is_stop):\n",
    "                # combine the dependent term and object, separated by an underscore\n",
    "                # e.g. travel agency ==> travel_agency\n",
    "                phrase = '{}_{}'.format(left_term,token.text)\n",
    "                phrases.append(phrase)\n",
    "    \n",
    "    # convert list of distinct phrases into a sentence\n",
    "    return ' '.join(set(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review data\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for sent in matched_sents_full[0:5]:\n",
    "    print(create_pos_phrases(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# apply the custom function to every element in the dataframe\n",
    "matches_df[0:5].sentences.apply(create_pos_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pandas Apply\n",
    "\n",
    "apply is an efficient and fast approach to 'apply' a function to every element in a row. applymap does the same to every element in the entire dataframe (e.g. convert all ints to floats)\n",
    "\n",
    "Example: https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_dataframes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a small dataframe with example data\n",
    "example_data = {'col1':range(0,3),'col2':range(3,6)}\n",
    "test_df = pd.DataFrame(example_data)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a built-in function to each element in a column\n",
    "test_df['col1'].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a custom function to every element in a column\n",
    "def add_five(row):\n",
    "    return row + 5\n",
    "\n",
    "test_df['col1'].apply(add_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply an annonomous function to every element in a column\n",
    "test_df['col1'].apply(lambda x: x+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a built-in function to every element in a dataframe \n",
    "test_df.applymap(float)  # applymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new empty column\n",
    "matches_df['create_pos_phrases'] = ''\n",
    "\n",
    "# apply the custom function to every element in the dataframe\n",
    "matches_df.loc[0:5, 'create_pos_phrases'] = matches_df[0:5].sentences.apply(create_pos_phrases)\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "\n",
    "\"A collocation is an expression consisting of two or more words that\n",
    "correspond to some conventional way of saying things. Or in the words\n",
    "of Firth (1957: 181): “Collocations of a given word are statements of the\n",
    "habitual or customary places of that word.” Collocations include noun\n",
    "phrases like strong tea and weapons of mass destruction, phrasal verbs like\n",
    "to make up, and other stock phrases like the rich and powerful. Particularly\n",
    "interesting are the subtle and not-easily-explainable patterns of word usage\n",
    "that native speakers all know: why we say a stiff breeze but not a stiff wind\n",
    "(while either a strong breeze or a strong wind is okay), or why we speak of\n",
    "broad daylight (but not bright daylight or narrow darkness)\n",
    "\n",
    "\n",
    "\n",
    "There are actually different definitions of the notion of collocation. Some\n",
    "authors in the computational and statistical literature define a collocation\n",
    "as two or more consecutive words with a special behavior, for example\n",
    "Choueka (1988):\n",
    "[A collocation is defined as] a sequence of two or more consecutive\n",
    "words, that has characteristics of a syntactic and semantic\n",
    "unit, and whose exact and unambiguous meaning or connotation\n",
    "cannot be derived directly from the meaning or connotation of its\n",
    "components. In most linguistically oriented research, a phrase\n",
    "can be a collocation even if it is not consecutive (as in the example knock\n",
    ". . . door). The following criteria are typical of linguistic treatments of collocations:\n",
    "\n",
    "**Non-compositionality**: The meaning of a collocation is not a straightforward\n",
    "composition of the meanings of its parts. Either the meaning\n",
    "is completely different from the free combination (as in the case of idioms\n",
    "like kick the bucket) or there is a connotation or added element of\n",
    "meaning that cannot be predicted from the parts. For example, white\n",
    "wine, white hair and white woman all refer to slightly different colors, so\n",
    "we can regard them as collocations. \n",
    "\n",
    "**Non-substitutability**: We cannot substitute near-synonyms for the\n",
    "components of a colloction. For example, we can’t say yellow wine\n",
    "instead of white wine even though yellow is as good a description of the\n",
    "color of white wine as white is (it is kind of a yellowish white).\n",
    "\n",
    "**Non-modifiability**: Many collocations cannot be freely modified with\n",
    "additional lexical material or through grammatical transformations.\n",
    "This is especially true for frozen expressions like idioms. For example,\n",
    "we can’t modify frog in to get a frog in one’s throat into to get an ugly\n",
    "frog in one’s throat although usually nouns like frog can be modified by\n",
    "adjectives like ugly. Similarly, going from singular to plural can make\n",
    "an idiom ill-formed, for example in people as poor as church mice.\"\n",
    "\n",
    "SOURCE: https://nlp.stanford.edu/fsnlp/promo/colloc.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a function that returns a window of size n over a given sentence. \n",
    "\n",
    "For the sentence **'rather than pay the fee'** return the following if the window is n=3:\n",
    "- ['rather', 'than', 'pay'],\n",
    "- ['than','pay','the']\n",
    "- ['pay', 'the','fee']\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example sentence\n",
    "sent = ' '.join(matches_df['sentences'][0:1]).split()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_windows(sentence, n=3):\n",
    "    \"create a sliding window over the n terms in a list of terms\"\n",
    "        \n",
    "    # create a window on the first n terms by slicing the sentence into the first n terms\n",
    "    window = \n",
    "    \n",
    "    # create a list to store all windows\n",
    "    # add the first window that was created above\n",
    "    sentence_windows = \n",
    "\n",
    "    # iterate through the rest of the terms of the sentence\n",
    "    # e.g. if n=3, then create a new window with terms 2 to 4\n",
    "    for :\n",
    "        # remove the first terms of the window and add the next term from the sentence\n",
    "        window = \n",
    "        # add the updated window to the master list\n",
    "        sentence_windows.append()\n",
    "\n",
    "    return sentence_windows\n",
    "\n",
    "# execute the function\n",
    "sentence_window = create_sentence_windows(sent, n=3)\n",
    "# view the first few results\n",
    "sentence_window[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the function for all sentences\n",
    "\n",
    "# create a list to store all windows\n",
    "sentence_window = []\n",
    "\n",
    "for sent in matches_df['sentences']:\n",
    "    # convert the sentence string into a list of terms\n",
    "    sent = sent.split()\n",
    "    \n",
    "    # create the sentence windows and append to the sentence_windows list\n",
    "    windows = create_sentence_windows(sent, n=3)\n",
    "    \n",
    "    # add each window to the sentence_window list\n",
    "    # iterate through windows to make each item in sentence window a window, not a list of windows\n",
    "    for window in windows:\n",
    "        sentence_window.append(window)\n",
    "\n",
    "# view the first five results\n",
    "sentence_window[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# create a defaultdict to keep track of common phrases\n",
    "window_count = defaultdict(int)\n",
    "\n",
    "for sent in sentence_window:\n",
    "    # remove stop words\n",
    "    sentence = [term for term in sent if term not in STOP_WORDS]\n",
    "    \n",
    "    # create a combination of terms\n",
    "    # e.g. (rather, than, pay) --> (rather,than), (than,pay), (rather,pay)\n",
    "    for combo in combinations(sentence, 2):\n",
    "        # convert the tuple to a term\n",
    "        # e.g. (rather, than) --> 'rather_than'\n",
    "        phrase = '_'.join(combo)\n",
    "        \n",
    "        # increment the count for the term each time it appears to identify the most common terms\n",
    "        window_count[phrase] += 1\n",
    "\n",
    "# sort to view the most common terms\n",
    "# the key (lambda x: x[1]) sorts by the count\n",
    "sorted(window_count.items(), key=lambda x: x[1], reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase (collocation) Detection\n",
    "\n",
    "Phrase modeling is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\\frac{count(A\\ B) - count_{min}}{count(A) * count(B)} > threshold$$\n",
    "\n",
    "- $count(A\\ B)$ is the number of times the tokens $A\\ B$ appear in the corpus in order\n",
    "- $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    "- $count(A)$ is the number of times token $A$ appears in the corpus\n",
    "- $count(B)$ is the number of times token $B$ appears in the corpus\n",
    "- $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so new york would become new_york). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as happy hour) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible gensim library to help us with phrase modeling — the Phrases class in particular.\n",
    "\n",
    "SOURCE: \n",
    "- https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb\n",
    "- https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scikit-learn API for Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_sents[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api.phrases import PhrasesTransformer\n",
    "\n",
    "sklearn_phrases = PhrasesTransformer(min_count=5, threshold=5)\n",
    "sklearn_phrases.fit(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review phrase matches\n",
    "phrases = []\n",
    "for terms in sklearn_phrases.transform(matched_sents):\n",
    "    for term in terms:\n",
    "        if term.count('_') >= 1:\n",
    "            phrases.append(term)\n",
    "\n",
    "print(set(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "common_terms = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**common_terms:** optional list of “stop words” that won’t affect frequency count of expressions containing them.\n",
    "- The common_terms parameter add a way to give special treatment to common terms (aka stop words) such that their presence between two words won’t prevent bigram detection. It allows to detect expressions like “bank of america” or “eye of the beholder”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gensim API\n",
    "A more complex API, though it is faster and has better integration with other gensim components (e.g. Phraser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(\n",
    "      matched_sents\n",
    "    , common_terms=common_terms\n",
    "    , min_count=5\n",
    "    , threshold=5\n",
    "    , scoring='default'\n",
    ")\n",
    "\n",
    "phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases Params\n",
    "\n",
    "- **scoring:** specifies how potential phrases are scored for comparison to the threshold setting. scoring can be set with either a string that refers to a built-in scoring function, or with a function with the expected parameter names. Two built-in scoring functions are available by setting scoring to a string:\n",
    "\n",
    "    - ‘default’: from “Efficient Estimaton of Word Representations in Vector Space” by Mikolov, et. al.: \n",
    "    \n",
    "$$\\frac{count(AB) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "    \n",
    "\n",
    "    - where N is the total vocabulary size.\n",
    "    - Thus, it is easier to exceed the threshold when the two words occur together often or when the two words are rare (i.e. small product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "\n",
    "bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phrases object still contains all the source text in memory. A gensim Phraser will remove this extra data to become smaller and somewhat faster than using the full Phrases model. To determine what data to remove, the Phraser ues the  results of the source model’s min_count, threshold, and scoring settings. (You can tamper with those & create a new Phraser to try other values.)\n",
    "\n",
    "SOURCE: https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_phrases(phraser, text_stream, num_underscores=2):\n",
    "    \"\"\" identify phrases from a text stream by searching for terms that\n",
    "        are separated by underscores and include at least num_underscores\n",
    "    \"\"\"\n",
    "    \n",
    "    phrases = []\n",
    "    for terms in phraser[text_stream]:\n",
    "        for term in terms:\n",
    "            if term.count('_') >= num_underscores:\n",
    "                phrases.append(term)\n",
    "    print(set(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_phrases(bigram, matched_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-gram phrase model\n",
    "\n",
    "We can place the text from the first phrase model into another Phrases object to create n-term phrase models. We can repear this process multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(\n",
    "      bigram[matched_sents]\n",
    "    , common_terms=common_terms\n",
    "    , min_count=1\n",
    "    , threshold=1\n",
    ")\n",
    "\n",
    "trigram = Phraser(phrases)\n",
    "\n",
    "print_phrases(trigram, bigram[matched_sents], num_underscores=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_num in [1]:\n",
    "    print('DOC NUMBER: {}\\n'.format(doc_num))\n",
    "    print('ORIGINAL SENTENT: {}\\n'.format(' '.join(matched_sents[doc_num])))\n",
    "    print('BIGRAM: {}\\n'.format(' '.join(bigram[matched_sents[doc_num]])))\n",
    "    print('TRIGRAM: {}'.format(' '.join(trigram[bigram[matched_sents[doc_num]]])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the cleaned text to a new file for later use\n",
    "\n",
    "#with open(CLEANED_TEXT_PATH, 'w') as f:\n",
    "#    for line in bigram[matched_sents]:\n",
    "#        line = ' '.join(line) + '\\n'\n",
    "#        line = line.encode('ascii', errors='ignore').decode('ascii')\n",
    "#        f.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
