{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines, RandomizedSearch, and Transformers\n",
    "\n",
    "##### Author: Alex Sherman | alsherman@deloitte.com\n",
    "\n",
    "##### Agenda:\n",
    "1. Pipelines\n",
    "2. Advanced Cross Validation (randomized Hyperparameter tuning)\n",
    "3. Feature Union (combining pipelines)\n",
    "4. Custom Transformers (customizing pipeline components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# feature engineering (vectorization and dimensionality reduction)\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, OneHotEncoder, LabelBinarizer, FunctionTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# machine learning experiment set-up\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import  RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, SelectPercentile\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn import datasets\n",
    "\n",
    "# visualization\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "\n",
    "REPORTER_DATA_PATH = config['DATA']['REPORTER_DATA_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTIVITY</th>\n",
       "      <th>ADMINISTERING_IC</th>\n",
       "      <th>FY</th>\n",
       "      <th>IC_NAME</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N01</td>\n",
       "      <td>HL</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL HEART, LUNG, AND BLOOD INSTITUTE</td>\n",
       "      <td>National Oral Health Information Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C06</td>\n",
       "      <td>RR</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL CENTER FOR RESEARCH RESOURCES</td>\n",
       "      <td>The Center for Oral/Head &amp;Neck Oncology Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C06</td>\n",
       "      <td>RR</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL CENTER FOR RESEARCH RESOURCES</td>\n",
       "      <td>Extramural Research Facilities Construction Constructio*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C06</td>\n",
       "      <td>RR</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL CENTER FOR RESEARCH RESOURCES</td>\n",
       "      <td>Cardiovascular Injury and Repair Research Facility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C06</td>\n",
       "      <td>RR</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL CENTER FOR RESEARCH RESOURCES</td>\n",
       "      <td>PAR04-122 Satellite Animal Facility Construction Project</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ACTIVITY ADMINISTERING_IC    FY                                    IC_NAME  \\\n",
       "0      N01               HL  2009  NATIONAL HEART, LUNG, AND BLOOD INSTITUTE   \n",
       "1      C06               RR  2009     NATIONAL CENTER FOR RESEARCH RESOURCES   \n",
       "2      C06               RR  2009     NATIONAL CENTER FOR RESEARCH RESOURCES   \n",
       "3      C06               RR  2009     NATIONAL CENTER FOR RESEARCH RESOURCES   \n",
       "4      C06               RR  2009     NATIONAL CENTER FOR RESEARCH RESOURCES   \n",
       "\n",
       "                                              PROJECT_TITLE  \n",
       "0                   National Oral Health Information Center  \n",
       "1          The Center for Oral/Head &Neck Oncology Research  \n",
       "2  Extramural Research Facilities Construction Constructio*  \n",
       "3        Cardiovascular Injury and Repair Research Facility  \n",
       "4  PAR04-122 Satellite Animal Facility Construction Project  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "      REPORTER_DATA_PATH\n",
    "    , encoding='latin-1'\n",
    "    , error_bad_lines=False\n",
    "    , nrows=1000\n",
    "    , usecols=['ADMINISTERING_IC', 'FY',  'IC_NAME', 'PROJECT_TITLE']\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTIVITY</th>\n",
       "      <th>ADMINISTERING_IC</th>\n",
       "      <th>FY</th>\n",
       "      <th>IC_NAME</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "      <th>NIAID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N01</td>\n",
       "      <td>HL</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL HEART, LUNG, AND BLOOD INSTITUTE</td>\n",
       "      <td>National Oral Health Information Center</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C06</td>\n",
       "      <td>RR</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL CENTER FOR RESEARCH RESOURCES</td>\n",
       "      <td>The Center for Oral/Head &amp;Neck Oncology Research</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C06</td>\n",
       "      <td>RR</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL CENTER FOR RESEARCH RESOURCES</td>\n",
       "      <td>Extramural Research Facilities Construction Constructio*</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C06</td>\n",
       "      <td>RR</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL CENTER FOR RESEARCH RESOURCES</td>\n",
       "      <td>Cardiovascular Injury and Repair Research Facility</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C06</td>\n",
       "      <td>RR</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL CENTER FOR RESEARCH RESOURCES</td>\n",
       "      <td>PAR04-122 Satellite Animal Facility Construction Project</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ACTIVITY ADMINISTERING_IC    FY                                    IC_NAME  \\\n",
       "0      N01               HL  2009  NATIONAL HEART, LUNG, AND BLOOD INSTITUTE   \n",
       "1      C06               RR  2009     NATIONAL CENTER FOR RESEARCH RESOURCES   \n",
       "2      C06               RR  2009     NATIONAL CENTER FOR RESEARCH RESOURCES   \n",
       "3      C06               RR  2009     NATIONAL CENTER FOR RESEARCH RESOURCES   \n",
       "4      C06               RR  2009     NATIONAL CENTER FOR RESEARCH RESOURCES   \n",
       "\n",
       "                                              PROJECT_TITLE  NIAID  \n",
       "0                   National Oral Health Information Center  False  \n",
       "1          The Center for Oral/Head &Neck Oncology Research  False  \n",
       "2  Extramural Research Facilities Construction Constructio*  False  \n",
       "3        Cardiovascular Injury and Repair Research Facility  False  \n",
       "4  PAR04-122 Satellite Animal Facility Construction Project  False  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up a binary classification problem\n",
    "df['NIAID'] = (df.IC_NAME == 'NATIONAL INSTITUTE OF ALLERGY AND INFECTIOUS DISEASES')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 6)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the row count\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the section names to determine if the section text will contain the term fee\n",
    "X = df['PROJECT_TITLE']\n",
    "y = df['NIAID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example ML Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.85      0.98      0.91       208\n",
      "       True       0.60      0.14      0.23        42\n",
      "\n",
      "avg / total       0.81      0.84      0.80       250\n",
      "\n",
      "Wall time: 208 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set up ML experiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# tfidf transform data\n",
    "tfidf = TfidfVectorizer()\n",
    "fit_vect = tfidf.fit_transform(X_train)\n",
    "\n",
    "# lsi transform data\n",
    "lsi = TruncatedSVD(n_components=100, random_state=42)\n",
    "fit_lsi = lsi.fit_transform(fit_vect)\n",
    "\n",
    "# build random forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(fit_lsi, y_train)\n",
    "\n",
    "# make predictions\n",
    "test_vect = tfidf.transform(X_test)\n",
    "test_lsi = lsi.transform(test_vect)\n",
    "y_pred = rf.predict(test_lsi)\n",
    "\n",
    "# evaluate prediction\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example INCORRECT(!!!) cross validation\n",
    "\n",
    "In the below process, we transform the data with tfidf and lsi before passing it to the RandomizedSearchCV\n",
    "\n",
    "Problems:\n",
    "1. All the data is passed to tfidf, so it learns the full vocabulary, instead of the vocabulary from only the training data\n",
    "2. All the data is passed to lsi, so it learns the full data, instead of the from only the training data\n",
    "3. We are only tuning the model hyperparameters instead of all transformer and feature selection hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# tfidf transform data\n",
    "tfidf = TfidfVectorizer()\n",
    "fit_vect = tfidf.fit_transform(X)  # wrong! - using all data \n",
    "\n",
    "# lsi transform data\n",
    "lsi = TruncatedSVD(random_state=42)\n",
    "fit_lsi = lsi.fit_transform(fit_vect)  # wrong! - using all data\n",
    "\n",
    "# hyperparameters to test\n",
    "param_dist = {'n_estimators':range(10,500,5)}  # limited to model hyperparameters\n",
    "\n",
    "# set cross validatation to test n random hyperparameters\n",
    "grid = RandomizedSearchCV(\n",
    "      RandomForestClassifier(random_state=42)\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=5\n",
    "    , cv=3\n",
    "    , refit='accuracy'\n",
    "    , scoring='accuracy'\n",
    "    , return_train_score=True\n",
    ")\n",
    "grid.fit(fit_lsi,y)\n",
    "\n",
    "# review results\n",
    "results = pd.DataFrame(grid.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.545347</td>\n",
       "      <td>0.034346</td>\n",
       "      <td>0.783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>435</td>\n",
       "      <td>{'n_estimators': 435}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.778443</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.461356</td>\n",
       "      <td>0.032657</td>\n",
       "      <td>0.783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>380</td>\n",
       "      <td>{'n_estimators': 380}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016092</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.526667</td>\n",
       "      <td>0.035334</td>\n",
       "      <td>0.781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>415</td>\n",
       "      <td>{'n_estimators': 415}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.778443</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023272</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.092663</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>0.780</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.772455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.786787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006309</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.112645</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>0.780</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95</td>\n",
       "      <td>{'n_estimators': 95}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.769461</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.786787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.545347         0.034346            0.783               1.0   \n",
       "3       0.461356         0.032657            0.783               1.0   \n",
       "2       0.526667         0.035334            0.781               1.0   \n",
       "1       0.092663         0.007001            0.780               1.0   \n",
       "4       0.112645         0.008679            0.780               1.0   \n",
       "\n",
       "  param_n_estimators                 params  rank_test_score  \\\n",
       "0                435  {'n_estimators': 435}                1   \n",
       "3                380  {'n_estimators': 380}                1   \n",
       "2                415  {'n_estimators': 415}                3   \n",
       "1                 70   {'n_estimators': 70}                4   \n",
       "4                 95   {'n_estimators': 95}                4   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.778443                 1.0           0.780781   \n",
       "3           0.781437                 1.0           0.783784   \n",
       "2           0.778443                 1.0           0.780781   \n",
       "1           0.772455                 1.0           0.780781   \n",
       "4           0.769461                 1.0           0.786787   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0                 1.0           0.789790                 1.0      0.032715   \n",
       "3                 1.0           0.783784                 1.0      0.016092   \n",
       "2                 1.0           0.783784                 1.0      0.023272   \n",
       "1                 1.0           0.786787                 1.0      0.006309   \n",
       "4                 1.0           0.783784                 1.0      0.003790   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.000489        0.004892              0.0  \n",
       "3        0.002054        0.001107              0.0  \n",
       "2        0.002626        0.002186              0.0  \n",
       "1        0.002159        0.005878              0.0  \n",
       "4        0.000937        0.007563              0.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Pipeline of transforms with a final estimator.\n",
    "\n",
    "Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a double underscore ‘__’. A step’s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines in sklearn\n",
    "\n",
    "\n",
    "Pipelines chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. \n",
    "\n",
    "Pipelines Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "\n",
    "Pipeline serves serveral purposes:\n",
    "\n",
    "- **Convenience and encapsulation**: You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "\n",
    "- **Joint parameter selection**: You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "- **Safety**: Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.)\n",
    "\n",
    "SOURCE:\n",
    "- [Pipeline: chaining estimators](http://scikit-learn.org/stable/modules/pipeline.html#pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define a pipeline\n",
    "# the pipeline includes a list of steps to complete\n",
    "# each step is a tuple with a name for the step and an uninstantiated class \n",
    "pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer(stop_words='english'))\n",
    "    , ('lsi', TruncatedSVD(random_state=42))\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# fitting a pipeline is the same as calling fit on every intermediate steps\n",
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "# predict using the model at the end of the pipeline\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.82      0.63      0.71       208\n",
      "       True       0.15      0.33      0.21        42\n",
      "\n",
      "avg / total       0.71      0.58      0.63       250\n",
      "\n",
      "accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "# evaluate prediction\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('accuracy: {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### view the results of the model on the testing datast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Type: FALSE POSITIVES\n",
      "                                                   PROJECT_TITLE  NIAID  pred\n",
      "521                              Role of TrpmI3 in Sensory Cells  False  True\n",
      "660  Role of Allopregnanolone in Mediating the Adverse Effect...  False  True\n",
      "411  The role of Sphingosine Kinase 1 in a Mouse Model of Chr...  False  True\n",
      "859  Effects of age-related cognitvie decline on attitudes to...  False  True\n",
      "76   Generation of tumor stem cell lines for directed therape...  False  True \n",
      "\n",
      "\n",
      "Result Type: FALSE NEGATIVES\n",
      "                                                   PROJECT_TITLE  NIAID   pred\n",
      "973  Investigation of a novel clonal variation pathway in Xen...   True  False\n",
      "938  The structure, dynamics and function of the pore-forming...   True  False\n",
      "371  Targeting Trypanosoma brucei S-adenosylmethionine decarb...   True  False\n",
      "947  Discovery of Small Molecules that Restore Methicillin Se...   True  False\n",
      "346  Molecular and Cellular Biology of an Ancient Allorecogni...   True  False \n",
      "\n",
      "\n",
      "Result Type: TRUE POSITIVES\n",
      "                                                   PROJECT_TITLE  NIAID  pred\n",
      "899  The Molecular Control of Cell Death in Staphylococcus au...   True  True\n",
      "883                              HS1 Function in Dendritic Cells   True  True\n",
      "902  Mechanistic analysis of T cell MTOC reorientation with a...   True  True\n",
      "370  HIV-1's Viral and Host Protein Interactions Determine th...   True  True\n",
      "866  The Role of CD8+ cells in the Development of Oral Tolerance   True  True \n",
      "\n",
      "\n",
      "Result Type: TRUE NEGATIVES\n",
      "                                                   PROJECT_TITLE  NIAID   pred\n",
      "737  Predicting Clostridium difficile infection:  Preoperativ...  False  False\n",
      "740  Diffusion Tensor Imaging to Define CNS Injury in Childre...  False  False\n",
      "678  Physiological reactivity in traumatized children receivi...  False  False\n",
      "626  A Functional Relationship between Specific NOS Isozymes ...  False  False\n",
      "513  Longitudinal Neurodevelopment of Auditory and Language C...  False  False \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optional - expand the column width to see more text\n",
    "pd.set_option('max_colwidth', 60)\n",
    "\n",
    "# combine X_test/y_test and add the prediction as a new column\n",
    "# to review model predictions\n",
    "df_test = pd.concat([X_test, y_test], axis=1) \n",
    "df_test['pred'] = y_pred  \n",
    "\n",
    "# define the quadrants of a confusion matrix\n",
    "fp = (df_test.NIAID == False) & (df_test.pred == True)\n",
    "fn = (df_test.NIAID == True) & (df_test.pred == False)\n",
    "tp = (df_test.NIAID == True) & (df_test.pred == True)\n",
    "tn = (df_test.NIAID == False) & (df_test.pred == False)\n",
    "\n",
    "confusion_matrix = [\n",
    "      ('FALSE POSITIVES',fp)\n",
    "    , ('FALSE NEGATIVES',fn)\n",
    "    , ('TRUE POSITIVES',tp)\n",
    "    , ('TRUE NEGATIVES',tn)\n",
    "]\n",
    "\n",
    "# view examples of each type of result in the confusion matrix\n",
    "# use this to help determine new useful feautes to improve the model\n",
    "for key, val in confusion_matrix:\n",
    "    print('Result Type: {}'.format(key))\n",
    "    print(df_test[val][['PROJECT_TITLE','NIAID','pred']][0:5], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...alty='l2', random_state=42,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the entire pipeline\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       " 'lsi': TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "        random_state=42, tol=0.0),\n",
       " 'tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a dict of each step in the pipeline\n",
    "# this is useful to select a single fit transformer or estimator\n",
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select only the tfidfvectorizer\n",
    "pipe.named_steps['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_char_ngrams', '_char_wb_ngrams', '_check_vocabulary', '_count_vocab', '_get_param_names', '_limit_features', '_sort_features', '_tfidf', '_validate_vocabulary', '_white_spaces', '_word_ngrams', 'analyzer', 'binary', 'build_analyzer', 'build_preprocessor', 'build_tokenizer', 'decode', 'decode_error', 'dtype', 'encoding', 'fit', 'fit_transform', 'fixed_vocabulary_', 'get_feature_names', 'get_params', 'get_stop_words', 'idf_', 'input', 'inverse_transform', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'norm', 'preprocessor', 'set_params', 'smooth_idf', 'stop_words', 'stop_words_', 'strip_accents', 'sublinear_tf', 'token_pattern', 'tokenizer', 'transform', 'use_idf', 'vocabulary', 'vocabulary_']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pipe.named_steps['tfidf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cardiac', 'care', 'caregiver', 'caregivers', 'carotid', 'cartilage', 'cascades', 'cashew', 'castration', 'catalysis', 'catalysts', 'catalytic', 'catalyzed', 'categorical', 'cation', 'cationic', 'caulobacter', 'causes', 'caveolin', 'cbt', 'cd', 'cd148', 'cd4', 'cd40', 'cd44', 'cd45', 'cd8', 'cdh1', 'celastrol', 'cell', 'cells', 'cellular', 'center', 'centered', 'central', 'cerebellar', 'cerebral', 'cervical', 'cessation', 'cf', 'cf3', 'cfs', 'cftr', 'challenge', 'changes', 'channel', 'channelopathies', 'channels', 'chaperone', 'chaperones', 'characterization', 'characterizing', 'charmm', 'cheating', 'chemical', 'chemistry', 'chemokines', 'chemoprevention', 'chemorepellent', 'chemotaxis', 'chemotherapeutic', 'chemotherapy', 'child', 'childhood', 'children', 'china', 'chk2', 'chlamydial', 'chloride', 'cholerae', 'cholesterol', 'cholinergic', 'chondroitin', 'chromans', 'chromatin', 'chronic', 'church', 'cigarette', 'ciliary', 'circadian', 'circuit', 'circuitry', 'circuits', 'circulating', 'cirrhosis', 'cisplatin', 'cla', 'class', 'classroom', 'clearance', 'cleaved', 'cleveland', 'click', 'climate', 'clinical', 'cll', 'cns', 'cnv', 'coactivator', 'cocaine']\n"
     ]
    }
   ],
   "source": [
    "# extract the features names from the fit tfidf in the pipeline\n",
    "feature_names = pipe.named_steps['tfidf'].get_feature_names()\n",
    "\n",
    "# print some features names learned by tfidf\n",
    "# ignore the first few hundred which are primarily numbers\n",
    "print(feature_names[300:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_get_param_names', 'algorithm', 'components_', 'explained_variance_', 'explained_variance_ratio_', 'fit', 'fit_transform', 'get_params', 'inverse_transform', 'n_components', 'n_iter', 'random_state', 'set_params', 'singular_values_', 'tol', 'transform']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pipe.named_steps['lsi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the variance ratio from the truncatedSVD (latent semantic indexing)\n",
    "pipe.named_steps['lsi'].n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_estimator_type', '_get_param_names', '_predict_proba_lr', 'class_weight', 'classes_', 'coef_', 'decision_function', 'densify', 'dual', 'fit', 'fit_intercept', 'get_params', 'intercept_', 'intercept_scaling', 'max_iter', 'multi_class', 'n_iter_', 'n_jobs', 'penalty', 'predict', 'predict_log_proba', 'predict_proba', 'random_state', 'score', 'set_params', 'solver', 'sparsify', 'tol', 'verbose', 'warm_start']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pipe.named_steps['clf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.57508478, 0.06905342]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['clf'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation with pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n",
    "\n",
    "It is possible and recommended to search the hyper-parameter space for the best cross validation score.\n",
    "\n",
    "Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, GridSearchCV exhaustively considers all parameter combinations, while RandomizedSearchCV can sample a given number of candidates from a parameter space with a specified distribution. After describing these tools we detail best practice applicable to both approaches.\n",
    "\n",
    "Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature.\n",
    "\n",
    "\n",
    "##### RandomizedSearchCV\n",
    "While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "A budget can be chosen independent of the number of parameters and possible values.\n",
    "Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified\n",
    "\n",
    "SOURCE:\n",
    "- [Tuning the hyper-parameters of an estimator](http://scikit-learn.org/stable/modules/grid_search.html#grid-search-tips)\n",
    "- [Publication - Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the pipeline\n",
    "pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer())\n",
    "    , ('lsi', TruncatedSVD(random_state=42))\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a parameter distribution which lists all the possible \n",
    "# hyperparameters to test\n",
    "\n",
    "param_dist = {\n",
    "       #  tfidf hyperparams\n",
    "         'tfidf__max_features': range(200,1000,10)\n",
    "       , 'tfidf__stop_words': [None, 'english']\n",
    "       , 'tfidf__ngram_range': [(1,1),(1,2), (1,3)]\n",
    " \n",
    "       #   lsi hyperparams\n",
    "       ,  'lsi__n_components': range(10,150)\n",
    "      \n",
    "       #   logistic regression hyperparams\n",
    "       ,  'clf__penalty':['l1','l2']\n",
    "       ,  'clf__C':np.linspace(1e3, 10, 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...alty='l2', random_state=42,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=5, n_jobs=1,\n",
       "          param_distributions={'clf__C': array([1000.,  990., ...,   20.,   10.]), 'tfidf__max_features': range(200, 1000, 10), 'lsi__n_components': range(10, 150), 'clf__penalty': ['l1', 'l2'], 'tfidf__stop_words': [None, 'english'], 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit='f1',\n",
       "          return_train_score=True, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# set cross validatation to test n random hyperparameters\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=5\n",
    "    , cv=3\n",
    "    , refit='f1'\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_clf__penalty</th>\n",
       "      <th>param_lsi__n_components</th>\n",
       "      <th>param_tfidf__max_features</th>\n",
       "      <th>param_tfidf__ngram_range</th>\n",
       "      <th>param_tfidf__stop_words</th>\n",
       "      <th>...</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.400662</td>\n",
       "      <td>0.010668</td>\n",
       "      <td>0.479222</td>\n",
       "      <td>0.790711</td>\n",
       "      <td>730</td>\n",
       "      <td>l1</td>\n",
       "      <td>118</td>\n",
       "      <td>910</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396947</td>\n",
       "      <td>0.861314</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.755853</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.754967</td>\n",
       "      <td>0.143133</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.066135</td>\n",
       "      <td>0.049925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.055347</td>\n",
       "      <td>0.004996</td>\n",
       "      <td>0.478635</td>\n",
       "      <td>0.667007</td>\n",
       "      <td>940</td>\n",
       "      <td>l2</td>\n",
       "      <td>84</td>\n",
       "      <td>880</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.660317</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.627832</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.078638</td>\n",
       "      <td>0.035038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.158996</td>\n",
       "      <td>0.004011</td>\n",
       "      <td>0.468473</td>\n",
       "      <td>0.722292</td>\n",
       "      <td>450</td>\n",
       "      <td>l1</td>\n",
       "      <td>98</td>\n",
       "      <td>310</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445860</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.463277</td>\n",
       "      <td>0.694006</td>\n",
       "      <td>0.496350</td>\n",
       "      <td>0.710963</td>\n",
       "      <td>0.105567</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.020942</td>\n",
       "      <td>0.028854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.141333</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>0.467485</td>\n",
       "      <td>0.766984</td>\n",
       "      <td>790</td>\n",
       "      <td>l2</td>\n",
       "      <td>116</td>\n",
       "      <td>980</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450331</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.452174</td>\n",
       "      <td>0.755853</td>\n",
       "      <td>0.015326</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.022987</td>\n",
       "      <td>0.023755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.137333</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.464810</td>\n",
       "      <td>0.692299</td>\n",
       "      <td>200</td>\n",
       "      <td>l1</td>\n",
       "      <td>93</td>\n",
       "      <td>280</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433121</td>\n",
       "      <td>0.737201</td>\n",
       "      <td>0.471338</td>\n",
       "      <td>0.664615</td>\n",
       "      <td>0.490066</td>\n",
       "      <td>0.675079</td>\n",
       "      <td>0.028939</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.023707</td>\n",
       "      <td>0.032037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.400662         0.010668         0.479222          0.790711   \n",
       "1       0.055347         0.004996         0.478635          0.667007   \n",
       "3       0.158996         0.004011         0.468473          0.722292   \n",
       "2       0.141333         0.011668         0.467485          0.766984   \n",
       "4       0.137333         0.007333         0.464810          0.692299   \n",
       "\n",
       "  param_clf__C param_clf__penalty param_lsi__n_components  \\\n",
       "0          730                 l1                     118   \n",
       "1          940                 l2                      84   \n",
       "3          450                 l1                      98   \n",
       "2          790                 l2                     116   \n",
       "4          200                 l1                      93   \n",
       "\n",
       "  param_tfidf__max_features param_tfidf__ngram_range param_tfidf__stop_words  \\\n",
       "0                       910                   (1, 2)                    None   \n",
       "1                       880                   (1, 1)                    None   \n",
       "3                       310                   (1, 1)                 english   \n",
       "2                       980                   (1, 3)                    None   \n",
       "4                       280                   (1, 2)                    None   \n",
       "\n",
       "        ...        split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0       ...                 0.396947            0.861314           0.558824   \n",
       "1       ...                 0.375000            0.712871           0.565217   \n",
       "3       ...                 0.445860            0.761905           0.463277   \n",
       "2       ...                 0.450331            0.800000           0.500000   \n",
       "4       ...                 0.433121            0.737201           0.471338   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.755853           0.482143            0.754967      0.143133   \n",
       "1            0.660317           0.496000            0.627832      0.002474   \n",
       "3            0.694006           0.496350            0.710963      0.105567   \n",
       "2            0.745098           0.452174            0.755853      0.015326   \n",
       "4            0.664615           0.490066            0.675079      0.028939   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.002867        0.066135         0.049925  \n",
       "1        0.001410        0.078638         0.035038  \n",
       "3        0.000015        0.020942         0.028854  \n",
       "2        0.002358        0.022987         0.023755  \n",
       "4        0.001885        0.023707         0.032037  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>mean_test_neg_log_loss</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>mean_train_f1</th>\n",
       "      <th>mean_train_neg_log_loss</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>std_test_neg_log_loss</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "      <th>std_train_f1</th>\n",
       "      <th>std_train_neg_log_loss</th>\n",
       "      <th>std_train_precision</th>\n",
       "      <th>std_train_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.630679</td>\n",
       "      <td>0.034325</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.443060</td>\n",
       "      <td>-1.038265</td>\n",
       "      <td>0.356108</td>\n",
       "      <td>0.595562</td>\n",
       "      <td>0.899510</td>\n",
       "      <td>0.772261</td>\n",
       "      <td>-0.262011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008908</td>\n",
       "      <td>0.045071</td>\n",
       "      <td>0.321118</td>\n",
       "      <td>0.019105</td>\n",
       "      <td>0.117891</td>\n",
       "      <td>0.014789</td>\n",
       "      <td>0.025530</td>\n",
       "      <td>0.048365</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.014237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.787655</td>\n",
       "      <td>0.031680</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.481412</td>\n",
       "      <td>-1.316375</td>\n",
       "      <td>0.444448</td>\n",
       "      <td>0.528298</td>\n",
       "      <td>0.929020</td>\n",
       "      <td>0.836580</td>\n",
       "      <td>-0.225716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018376</td>\n",
       "      <td>0.028970</td>\n",
       "      <td>0.937828</td>\n",
       "      <td>0.039830</td>\n",
       "      <td>0.038005</td>\n",
       "      <td>0.028982</td>\n",
       "      <td>0.060335</td>\n",
       "      <td>0.058037</td>\n",
       "      <td>0.090203</td>\n",
       "      <td>0.006861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.137655</td>\n",
       "      <td>0.049335</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.478477</td>\n",
       "      <td>-0.840449</td>\n",
       "      <td>0.427495</td>\n",
       "      <td>0.544843</td>\n",
       "      <td>0.933514</td>\n",
       "      <td>0.843475</td>\n",
       "      <td>-0.189698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019775</td>\n",
       "      <td>0.048953</td>\n",
       "      <td>0.192531</td>\n",
       "      <td>0.042043</td>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.023278</td>\n",
       "      <td>0.047787</td>\n",
       "      <td>0.059772</td>\n",
       "      <td>0.066984</td>\n",
       "      <td>0.010481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.039333</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.417790</td>\n",
       "      <td>-0.644114</td>\n",
       "      <td>0.315703</td>\n",
       "      <td>0.640250</td>\n",
       "      <td>0.754501</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>-0.495129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061380</td>\n",
       "      <td>0.028820</td>\n",
       "      <td>0.113245</td>\n",
       "      <td>0.039177</td>\n",
       "      <td>0.081966</td>\n",
       "      <td>0.021482</td>\n",
       "      <td>0.031423</td>\n",
       "      <td>0.023696</td>\n",
       "      <td>0.028768</td>\n",
       "      <td>0.031248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.071001</td>\n",
       "      <td>0.022335</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.459247</td>\n",
       "      <td>-2.257947</td>\n",
       "      <td>0.441006</td>\n",
       "      <td>0.489021</td>\n",
       "      <td>0.953021</td>\n",
       "      <td>0.886897</td>\n",
       "      <td>-0.143777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.040131</td>\n",
       "      <td>1.421211</td>\n",
       "      <td>0.039331</td>\n",
       "      <td>0.085690</td>\n",
       "      <td>0.030040</td>\n",
       "      <td>0.071121</td>\n",
       "      <td>0.088789</td>\n",
       "      <td>0.115403</td>\n",
       "      <td>0.010481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_accuracy  mean_test_f1  \\\n",
       "0       1.630679         0.034325               0.738      0.443060   \n",
       "1       0.787655         0.031680               0.797      0.481412   \n",
       "2       0.137655         0.049335               0.789      0.478477   \n",
       "3       0.033000         0.039333               0.679      0.417790   \n",
       "4       1.071001         0.022335               0.797      0.459247   \n",
       "\n",
       "   mean_test_neg_log_loss  mean_test_precision  mean_test_recall  \\\n",
       "0               -1.038265             0.356108          0.595562   \n",
       "1               -1.316375             0.444448          0.528298   \n",
       "2               -0.840449             0.427495          0.544843   \n",
       "3               -0.644114             0.315703          0.640250   \n",
       "4               -2.257947             0.441006          0.489021   \n",
       "\n",
       "   mean_train_accuracy  mean_train_f1  mean_train_neg_log_loss  \\\n",
       "0             0.899510       0.772261                -0.262011   \n",
       "1             0.929020       0.836580                -0.225716   \n",
       "2             0.933514       0.843475                -0.189698   \n",
       "3             0.754501       0.521654                -0.495129   \n",
       "4             0.953021       0.886897                -0.143777   \n",
       "\n",
       "         ...         std_test_accuracy  std_test_f1 std_test_neg_log_loss  \\\n",
       "0        ...                  0.008908     0.045071              0.321118   \n",
       "1        ...                  0.018376     0.028970              0.937828   \n",
       "2        ...                  0.019775     0.048953              0.192531   \n",
       "3        ...                  0.061380     0.028820              0.113245   \n",
       "4        ...                  0.017156     0.040131              1.421211   \n",
       "\n",
       "  std_test_precision std_test_recall std_train_accuracy std_train_f1  \\\n",
       "0           0.019105        0.117891           0.014789     0.025530   \n",
       "1           0.039830        0.038005           0.028982     0.060335   \n",
       "2           0.042043        0.064134           0.023278     0.047787   \n",
       "3           0.039177        0.081966           0.021482     0.031423   \n",
       "4           0.039331        0.085690           0.030040     0.071121   \n",
       "\n",
       "  std_train_neg_log_loss std_train_precision  std_train_recall  \n",
       "0               0.048365            0.036539          0.014237  \n",
       "1               0.058037            0.090203          0.006861  \n",
       "2               0.059772            0.066984          0.010481  \n",
       "3               0.023696            0.028768          0.031248  \n",
       "4               0.088789            0.115403          0.010481  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set several evaluation metrics to collect during RandomizedSearch\n",
    "scoring = ['accuracy','neg_log_loss','precision','recall','f1']\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=5\n",
    "    , cv=3\n",
    "    , refit='f1'\n",
    "    , scoring=scoring  # track multiple evaluation metrics\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "grid.fit(X,y)\n",
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distributions for RandomizedGridSearch hyperparameters sampling\n",
    "\n",
    "##### scipy.stats.distributions\n",
    "\n",
    "[e.g. scipy.stats.lognorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html#scipy.stats.lognorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "dist = np.linspace(lognorm.ppf(0.01, 1), lognorm.ppf(0.99, 1), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e0d70133c8>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e0d6f79748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(dist, lognorm.pdf(dist, 1), 'r-', lw=5, alpha=0.6, label='lognorm pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_clf__C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.250306</td>\n",
       "      <td>7.21489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.249905</td>\n",
       "      <td>7.29611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.249905</td>\n",
       "      <td>7.47886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.248288</td>\n",
       "      <td>5.73255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.238826</td>\n",
       "      <td>0.422547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.242138</td>\n",
       "      <td>0.209334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.248288</td>\n",
       "      <td>6.35188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.246438</td>\n",
       "      <td>2.25008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.243840</td>\n",
       "      <td>1.49876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.250306</td>\n",
       "      <td>7.14382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_clf__C\n",
       "0         0.250306      7.21489\n",
       "1         0.249905      7.29611\n",
       "2         0.249905      7.47886\n",
       "3         0.248288      5.73255\n",
       "4         0.238826     0.422547\n",
       "5         0.242138     0.209334\n",
       "6         0.248288      6.35188\n",
       "7         0.246438      2.25008\n",
       "8         0.243840      1.49876\n",
       "9         0.250306      7.14382"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scipy.stats.lognorm\n",
    "param_dist = {'clf__C':dist}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=10\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "grid.fit(X,y)\n",
    "\n",
    "# selected eval columns\n",
    "eval_cols = ['mean_test_score','param_clf__C']\n",
    "pd.DataFrame(grid.cv_results_)[eval_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomizedGridSearch Robustness to failure with error_score\n",
    "\n",
    "Some parameter settings may result in a failure to fit one or more folds of the data. By default, this will cause the entire search to fail, even if some parameter settings could be fully evaluated. \n",
    "\n",
    "**Setting error_score=0** (or =np.NaN) will make the procedure robust to such failure, issuing a warning and setting the score for that fold to 0 (or NaN), but completing the search.\n",
    "\n",
    "NOTE: Without setting this parameter a single hyperparameter mismatch could cause an entire training routine to fail. For example, if you set a range of max_features with the lowest value having a lower option than another hyperparameter such as lsi or pca components. \n",
    "\n",
    "Speaking from personal experience, I have had 8-hour training routines fail in the last hour due to this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 110 >= 100',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 110 >= 100',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 120 >= 100',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 120 >= 100',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 120 >= 120',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 120 >= 120',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_lsi__n_components</th>\n",
       "      <th>param_tfidf__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110</td>\n",
       "      <td>100</td>\n",
       "      <td>{'tfidf__max_features': 100, 'lsi__n_components': 110}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.040997</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.389897</td>\n",
       "      <td>0.666175</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>{'tfidf__max_features': 120, 'lsi__n_components': 110}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.376569</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.638655</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>7.152557e-07</td>\n",
       "      <td>0.013328</td>\n",
       "      <td>0.027519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>{'tfidf__max_features': 100, 'lsi__n_components': 120}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>{'tfidf__max_features': 120, 'lsi__n_components': 120}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.011001            0.000         0.000000          0.000000   \n",
       "1       0.040997            0.006         0.389897          0.666175   \n",
       "2       0.012000            0.000         0.000000          0.000000   \n",
       "3       0.016002            0.000         0.000000          0.000000   \n",
       "\n",
       "  param_lsi__n_components param_tfidf__max_features  \\\n",
       "0                     110                       100   \n",
       "1                     110                       120   \n",
       "2                     120                       100   \n",
       "3                     120                       120   \n",
       "\n",
       "                                                   params  rank_test_score  \\\n",
       "0  {'tfidf__max_features': 100, 'lsi__n_components': 110}                2   \n",
       "1  {'tfidf__max_features': 120, 'lsi__n_components': 110}                1   \n",
       "2  {'tfidf__max_features': 100, 'lsi__n_components': 120}                2   \n",
       "3  {'tfidf__max_features': 120, 'lsi__n_components': 120}                2   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.000000            0.000000           0.000000   \n",
       "1           0.376569            0.693694           0.403226   \n",
       "2           0.000000            0.000000           0.000000   \n",
       "3           0.000000            0.000000           0.000000   \n",
       "\n",
       "   split1_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0            0.000000      0.000999    0.000000e+00        0.000000   \n",
       "1            0.638655      0.003005    7.152557e-07        0.013328   \n",
       "2            0.000000      0.001993    0.000000e+00        0.000000   \n",
       "3            0.000000      0.000997    0.000000e+00        0.000000   \n",
       "\n",
       "   std_train_score  \n",
       "0         0.000000  \n",
       "1         0.027519  \n",
       "2         0.000000  \n",
       "3         0.000000  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer())\n",
    "    , ('lsi', TruncatedSVD(random_state=42))\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# set a parameter distribution which lists all the possible hyperparameters to test\n",
    "param_dist = {\n",
    "       #  tfidf hyperparams\n",
    "         'tfidf__max_features': [100,120]\n",
    "       #   lsi hyperparams\n",
    "       ,  'lsi__n_components': [110, 120]\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=4\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    "    , error_score=0\n",
    ")\n",
    "\n",
    "grid.fit(X,y)\n",
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross-validation (CV)\n",
    "\n",
    "Nested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score.\n",
    "\n",
    "Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus “leak” into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model.\n",
    "\n",
    "To avoid this problem, nested CV effectively uses a series of train/validation/test set splits. In the inner loop (here executed by GridSearchCV), the score is approximately maximized by fitting a model to each training set, and then directly maximized in selecting (hyper)parameters over the validation set. In the outer loop (here in cross_val_score), generalization error is estimated by averaging test set scores over several dataset splits.\n",
    "\n",
    "SOURCE:\n",
    "- [Nested versus non-nested cross validation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py)\n",
    "- [Nested Cross Validation: When Cross Validation Isn’t Enough](https://www.elderresearch.com/blog/nested-cross-validation)\n",
    "- [Nested Cross Validation - sklearn](https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44511100394044806"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "param_dist = {\n",
    "    'tfidf__max_features': [150]\n",
    "  , 'lsi__n_components': [110, 120, 130]\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeatureUnion combines several transformer objects into a new transformer that combines their output. A FeatureUnion takes a list of transformer objects. During fitting, each of these is fit to the data independently. For transforming data, the transformers are applied in parallel, and the sample vectors they output are concatenated end-to-end into larger vectors.\n",
    "\n",
    "FeatureUnion serves the same purposes as Pipeline - convenience and joint parameter estimation and validation.\n",
    "\n",
    "FeatureUnion and Pipeline can be combined to create complex models.\n",
    "\n",
    "(A FeatureUnion has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are the caller’s responsibility.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tfidf_pipe', Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_ra...uncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "       random_state=42, tol=0.0))]))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "tfidf_pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer())\n",
    "    , ('lsi', TruncatedSVD(n_components=100, random_state=42))\n",
    "])\n",
    "\n",
    "count_pipe = Pipeline([\n",
    "      ('count', CountVectorizer())\n",
    "    , ('lsi', TruncatedSVD(n_components=100, random_state=42))\n",
    "])\n",
    "\n",
    "FeatureUnion([('tfidf_pipe', tfidf_pipe), ('count_pipe',count_pipe)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3474564022169966"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a feature union to a pipeline\n",
    "pipe = Pipeline([\n",
    "      ('feature_union', FeatureUnion([\n",
    "          ('tfidf_pipe', tfidf_pipe)\n",
    "        , ('count_pipe',count_pipe)\n",
    "    ]))\n",
    "    , ('clf', LogisticRegression(C=1, random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    'feature_union__count_pipe__count__max_features': [200,500,1000,5000]\n",
    "  , 'feature_union__tfidf_pipe__tfidf__max_features': [200,500,1000,5000]\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4088221355958113"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a feature union to a pipeline\n",
    "# provide different weights to each step in the pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_union', FeatureUnion([\n",
    "          ('tfidf_pipe', tfidf_pipe)\n",
    "        , ('count_pipe', count_pipe)\n",
    "    # add transformer weights\n",
    "    ], transformer_weights={\n",
    "          \"tfidf_pipe\": 5\n",
    "        , \"count_pipe\": 1\n",
    "        }))\n",
    "    , ('clf', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADMINISTERING_IC</th>\n",
       "      <th>FY</th>\n",
       "      <th>IC_NAME</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "      <th>NIAID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HL</td>\n",
       "      <td>2009</td>\n",
       "      <td>NATIONAL HEART, LUNG, AND BLOOD INSTITUTE</td>\n",
       "      <td>National Oral Health Information Center</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ADMINISTERING_IC    FY                                    IC_NAME  \\\n",
       "0               HL  2009  NATIONAL HEART, LUNG, AND BLOOD INSTITUTE   \n",
       "\n",
       "                             PROJECT_TITLE  NIAID  \n",
       "0  National Oral Health Information Center  False  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update X to include more than 1 feature\n",
    "X = df[['PROJECT_TITLE','FY']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ItemSelector\n",
    "Create a custom transformer to select a subset of features inside a pipeline\n",
    "\n",
    "Transformers must have a fit and transform method: \n",
    "\n",
    "**fit method:** used to learn any parameters from the traning data that will be applied to the testing data. As an example, learning the mean or standard deviation in a StandardScaler or learning the vocabulary in a CountVectorizer. If the transformer does not need to learn any parameters, then the fit must still exist (as required to keep the scikit-learn api consistent), but can simple have a return self as the entire code.\n",
    "\n",
    "**transform:** apply any transformations to the data, such as applying a z-score stanardization in StandardScaler, filtering to relevant terminology in a CountVectoirzer, or selecting a subset of features in the below ItemSelector\n",
    "\n",
    "**init method:** optional - used to store any parameters needed for other methods in the transformer, such as storing the variable names used to select the subset of features in the below ItemSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Select a subset of features as a step in a sklearn pipeline \"\"\"\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2517430322308371"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example pipeline to select only a single text field\n",
    "\n",
    "pipe = Pipeline([\n",
    "       ('select_text', ItemSelector(key='PROJECT_TITLE'))\n",
    "    ,  ('tfidf', TfidfVectorizer())\n",
    "    ,  ('lsi', TruncatedSVD(random_state=42))\n",
    "    ,  ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {'tfidf__max_features': [200,500,1000,5000]}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4776987289548169"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pipe to apply separate preprocessing to different text fields\n",
    "# create 50 components for the section name and 100 for the section text\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_union', FeatureUnion([\n",
    "          ('section_name', Pipeline([\n",
    "               ('select_text', ItemSelector(key='PROJECT_TITLE'))\n",
    "            ,  ('tfidf', TfidfVectorizer())\n",
    "            ,  ('lsi', TruncatedSVD(n_components=50, random_state=42))\n",
    "          ])),\n",
    "          ('section_text', Pipeline([\n",
    "               ('select_text', ItemSelector(key='PROJECT_TITLE'))\n",
    "            ,  ('tfidf', TfidfVectorizer())\n",
    "            ,  ('lsi', TruncatedSVD(n_components=100, random_state=42))\n",
    "          ]))\n",
    "        ])\n",
    "    )\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {'feature_union__section_name__tfidf__max_features': [200,500,1000,5000]}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparseMatrixTransformer\n",
    "\n",
    "Transform numeric data into a sparse representation to allow numeric and text fields to be combined together in a pipeline with a FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "class SparseMatrixTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Converts a dense matrix into a sparse matrix \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # convert dense feature(s) into a sparse matrix\n",
    "        return sp.sparse.csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4777744817594554"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a feature union that includes a numeric feature\n",
    "# in order to combine numeric and text features, you must convert\n",
    "# the dense and sparse matrices to match\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_union', FeatureUnion([\n",
    "          ('text', Pipeline([\n",
    "               ('select_text', ItemSelector(key='PROJECT_TITLE'))\n",
    "            ,  ('tfidf', TfidfVectorizer())\n",
    "            ,  ('lsi', TruncatedSVD(n_components=100, random_state=42))\n",
    "          ]))\n",
    "        , ('numeric', Pipeline([\n",
    "               ('select_len', ItemSelector(key=['FY']))\n",
    "             , ('sparse', SparseMatrixTransformer())\n",
    "          ]))\n",
    "        ])\n",
    "    )\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {'feature_union__text__tfidf__max_features': [200,500,1000,5000]}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline optimizations - cache intermediate steps with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['PROJECT_TITLE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regular Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# regular pipeline\n",
    "pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer())\n",
    "    , ('lsi', TruncatedSVD(random_state=42))\n",
    "    , ('clf', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# hyperparameters\n",
    "dist = np.linspace(lognorm.ppf(0.01, 1), lognorm.ppf(0.99, 1), 1000)\n",
    "param_dist = {\n",
    "         'tfidf__max_features': range(200,1000,10)\n",
    "       , 'tfidf__stop_words': [None, 'english']\n",
    "       , 'tfidf__ngram_range': [(1,1),(1,2), (1,3)]\n",
    "       , 'lsi__n_components': range(10,150)\n",
    "       , 'clf__penalty':['l1','l2']\n",
    "       , 'clf__C':dist\n",
    "       , 'clf__class_weight':['balanced']\n",
    "}\n",
    "\n",
    "# use cross validation\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=1000\n",
    "    , cv=3\n",
    "    , refit='f1'\n",
    "    , scoring='f1'\n",
    "    , error_score=0\n",
    ")\n",
    "\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tempdir: C:\\Users\\ALSHER~1\\AppData\\Local\\Temp\\tmpsewtklrh\n"
     ]
    }
   ],
   "source": [
    "from tempfile import mkdtemp\n",
    "\n",
    "# create a temp dir to store fit data from sklearn pipeline\n",
    "cachedir = mkdtemp()\n",
    "print('tempdir: {}'.format(cachedir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# add memory=cachedir to save fitting in intermediate steps \n",
    "# (e.g. fit tfidf and lsi only once )\n",
    "pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer())\n",
    "    , ('lsi', TruncatedSVD(random_state=42))\n",
    "    , ('clf', LogisticRegression(random_state=42))\n",
    "], memory=cachedir)\n",
    "\n",
    "# hyperparameters\n",
    "dist = np.linspace(lognorm.ppf(0.01, 1), lognorm.ppf(0.99, 1), 1000)\n",
    "param_dist = {\n",
    "         'tfidf__max_features': range(200,1000,10)\n",
    "       , 'tfidf__stop_words': [None, 'english']\n",
    "       , 'tfidf__ngram_range': [(1,1),(1,2), (1,3)]\n",
    "       , 'lsi__n_components': range(10,150)\n",
    "       , 'clf__penalty':['l1','l2']\n",
    "       , 'clf__C':dist\n",
    "       , 'clf__class_weight':['balanced']\n",
    "}\n",
    "\n",
    "# use cross validation\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=1000\n",
    "    , cv=3\n",
    "    , refit='f1'\n",
    "    , scoring='f1'\n",
    "    , error_score=0\n",
    ")\n",
    "\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the cache directory when you don't need it anymore\n",
    "from shutil import rmtree\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Persistence\n",
    "\n",
    "After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filename.pkl']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(grid, 'filename.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = joblib.load('filename.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security & maintainability limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle (and joblib by extension), has some issues regarding maintainability and security. Because of this,\n",
    "\n",
    "Never unpickle untrusted data as it could lead to malicious code being executed upon loading.\n",
    "While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results.\n",
    "In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:\n",
    "\n",
    "The training data, e.g. a reference to a immutable snapshot\n",
    "The python source code used to generate the model\n",
    "The versions of scikit-learn and its dependencies\n",
    "The cross validation score obtained on the training data\n",
    "This should make it possible to check that the cross-validation score is in the same range as before.\n",
    "\n",
    "Since a model internal representation may be different on two different architectures, dumping a model on one architecture and loading it on another architecture is not supported.\n",
    "\n",
    "If you want to know more about these issues and explore other possible serialization methods, please refer to [this talk by Alex Gaynor - pickles are for delis not software](http://pyvideo.org/pycon-us-2014/pickles-are-for-delis-not-software.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
